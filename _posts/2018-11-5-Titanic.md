---
layout: post
title: Titanic_ML
projects: true
Category: project
author: ChrisKoniniec
---
## Titanic Machine Learning Classification
---
In this post I will show my work for one of the most common datasets in recent history,
predicting survivability for passengers on the Titanic. The link to the Kaggle competition can be shown below.

[Kaggle Titanic Dataset](https://www.kaggle.com/c/titanic)

Lets begin!
I start with importing all the libraries I may need for the project

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
```

Since I know we'll be doing a classification, I'll define my classification algorithms and put them in a list
```
gbc = GradientBoostingClassifier()
r = RandomForestClassifier()
d = DecisionTreeClassifier()
l = LogisticRegression()
k = KNeighborsClassifier()
g = GaussianNB()
b = BernoulliNB()

#list of algorithms and names for our function later on
algorithms = [gbc, r, d, l, k, g, b]
names= ['Gradient Boosting', 'Random Forest', 'Decision Tree','Logisic Regression','K Nearest', 'GaussianNB', 'BernoulliNB']
```
Next, we'll define the function that we'll use to figure out which classifier will best predict the data

I like to put these at the beginning so I can easily refer back to the function if I need to later
```
def tDMassess(X, y, algorithms= algorithms, names = names):
    #train the data
    for i in range(len(algorithms)):
        algorithms[i] = algorithms[i].fit(X,y)
    #print metrics
    accuracy = []
    precision = []
    recall = []
    f1 = []
    for i in range(len(algorithms)):
        print(i)
        accuracy.append(accuracy_score(y, algorithms[i].predict(X)))
        print(accuracy)
        precision.append(precision_score(y, algorithms[i].predict(X)))
        print(precision)
        recall.append(recall_score(y, algorithms[i].predict(X)))
        print(recall)
        f1.append(f1_score(y, algorithms[i].predict(X)))
        print(f1)
        print('next loop')
    metrics = pd.DataFrame(columns = ['Accuracy', 'Precision', 'Recall', 'F1'], index= names)
    metrics['Accuracy'] = accuracy
    metrics['Precision'] = precision
    metrics['Recall'] = recall
    metrics['F1'] = f1
    return metrics
```
Now let's import our data and do some exploratory data analysis
```
training = pd.read_csv('C:\\Users\\ckoni\\Desktop\\DevMastersWork\\Day 6 Files\\all\\train.csv')
test = pd.read_csv('C:\\Users\\ckoni\\Desktop\\DevMastersWork\\Day 6 Files\\all\\test.csv')

print(training.head())
print(test.head())
```

Here I'm just going to take a look at some of the null values that will give us trouble later
```
test.isnull().sum()
```
```
test[test.Fare.isnull()]
```
```
test[test['Pclass'] == 3].mean()
```
I'll just fill in that null value with the mean Fare for 3rd class
```
test.loc[152, 'Fare'] = 12.459678
```
```
#combine the data for eda and feature engineering
data = pd.concat([training, test])
```
```
data.describe()
```
```
data.isnull().sum()
```
There are a few columns with null values that we'll have to take care of.

Taking a look at some graphs gives us a good idea about which features(columns) will be important.
```
sns.set(style="darkgrid")
_ = sns.swarmplot(x="Sex", y="Pclass", hue="Survived", data= data)
```

```
sns.set(style="darkgrid")
_ = sns.swarmplot(x="Sex", y="Age", hue="Survived", data= data)
```

As you can see, Age and Sex will be pretty important to infer who survives. At first, I applied the mean age to everyone with null values for age. A bit later, I found a better way to estimate age.

```
data[data['Age'].isnull()]
```
If we take a look at the passenger's names, we can infer that people with similar titles will have similar ages.
```
names1 = pd.DataFrame(data['Name'].str.split(',', n=1).tolist(), columns = ['surname', 'given'])
names2 = pd.DataFrame(names1['given'].str.split('.', n=1).tolist(), columns = ['prefix', 'given'])
prefix = names2['prefix']
prefix = pd.DataFrame(prefix)
prefix.columns = ['title']
data['title'] = prefix
data.head()
```
```
data['Age'] = data.groupby('title').transform(lambda x: x.fillna(x.mean()))

data.Age.isnull().sum()
```
```
data.reset_index()
```

Time for some feature engineering and then we'll run our first assessment
The 'Cabin' column has a ton of null values, so we'll be dropping it
```
data.drop('Cabin', inplace = True, axis=1)
```
There are also a couple nulls in 'Embarked'. They dont seem too important so I'll fill them with the most common value
```
data.Embarked.fillna('S', inplace = True)
```
Next, we'll use get_dummies to transform categorical variables into more machine-readable columns
```
Pclassdumm = pd.get_dummies(data['Pclass'], drop_first = True)
data = pd.concat([data, Pclassdumm], axis= 1)
data.drop('Pclass', inplace = True, axis= 1)
```
```
sexdumm = pd.get_dummies(data['Sex'], drop_first = True)
data = pd.concat([data, sexdumm], axis= 1)
data.drop('Sex', inplace = True, axis=1)
```
```
Embardumm = pd.get_dummies(data['Embarked'], drop_first = True)
data = pd.concat([data, Embardumm], axis= 1)
data.drop('Embarked', inplace = True, axis=1)
```
Now we'll drop the columns that we dont need ( mostly the object columns)
```
data.drop('Name', inplace = True, axis = 1)
data.drop('PassengerId', inplace = True, axis = 1)
data.drop('title', inplace = True, axis = 1)
data.drop('Ticket', inplace = True, axis = 1)
```
Since we saw earlier that Women and children tended to survive more often, we'll make a boolean column that determines if a passenger is a child or not.
```
data['Children'] = data['Age'] < 18
```

All done with feature engineering for now, time to re-split our data and run the assessment!
```
#selecting all the data that wont be 'tested' by Kaggle
train = data[data['Survived'].notnull()]
```
```
target = train[['Survived']]
features = train.drop('Survived', axis= 1)
```
```
tDMassess(features, target)
```
So we can tell from this assessment that Gradient Boosting or Random Forest would be the best methods to use to predict.
Now we'll re-split the full dataset and use Gradient Boosting on it
```
training = data[:891]
testing = data[891:]
```
```
testing.drop('Survived', inplace = True, axis= 1)
```
```
#save the survival predictions as a variable
predict = gbc.predict(testing)
```
```
#add the predictions as the survival values in the ORIGINAL test data set from the beginning
test["Survived"] = predict.astype('int')
```
```
#Kaggle only needs the PassengerId and our predictions
test = test[['PassengerId', 'Survived']]
```
```
test.to_csv('titanic_predict.csv', index = False)
```
Then we submit to Kaggle! Using this code I got about 78% prediction rate which is in the top 23 percentile.
